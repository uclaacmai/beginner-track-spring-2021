{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Guided Project Skeleton - Logistic Regression (Object-Oriented).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLLBDQBvZ4HR"
      },
      "source": [
        "# $$\\textbf{Welcome to Beginner Track - Workshop 7}$$\n",
        "## Today, we are going to implement a logistic regression model AND a multiclass model from scratch (using numpy)\n",
        "Attendance code: Brazil \\\\\n",
        "Feedback form: https://forms.gle/AtuL9181NbSZ5FV4A\n",
        "\n",
        "Based on: https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece\n",
        "\n",
        "## Join Projects for cool hands-on AI work\n",
        "\n",
        "Projects FAQ: https://docs.google.com/document/d/1zeRmCB4nK5tY4VgugOadoqyW3ZRM5BLAJX_7tZ-UKDg/edit?usp=sharing.\n",
        "\n",
        "Projects Interest Form!!: https://docs.google.com/forms/d/e/1FAIpQLScwikVsmhoSCmzSKgReq0Xk3MboiCOI6CXw_TUydwpTZ_kegA/viewform?usp=sf_link\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x0Tv2bUgL7D"
      },
      "source": [
        "# import modules\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RKs68B4OsNa"
      },
      "source": [
        "# set the random seed\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDDGztxJgub0"
      },
      "source": [
        "# create data\n",
        "x11 = np.random.normal(loc=3, size=100)\n",
        "x12 = np.random.normal(loc=5, size=100)\n",
        "\n",
        "x0 = np.stack((x11, x12)).T\n",
        "\n",
        "x21 = np.random.normal(loc=6, size=100)\n",
        "x22 = np.random.normal(loc=7, size=100)\n",
        "\n",
        "x1 = np.stack((x21, x22)).T\n",
        "\n",
        "X_ori = np.zeros((200, 2))\n",
        "y_ori = np.zeros(200)\n",
        "y_ori[100:] = 1\n",
        "X_ori[:100] = x0\n",
        "X_ori[100:] = x1\n",
        "\n",
        "indicies = np.random.choice(np.arange(200), replace=False, size=200)\n",
        "X = np.zeros((200, 2))\n",
        "y = np.zeros(200)\n",
        "X = X_ori[indicies]\n",
        "y = y_ori[indicies]\n",
        "\n",
        "plt.scatter(x11, x12, label='Class 0')\n",
        "plt.scatter(x21, x22, label='Class 1')\n",
        "plt.legend()\n",
        "\n",
        "# augment with 1's\n",
        "\n",
        "# X = np.hstack((np.ones((200, 1)), X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NETOjmqWg5D-"
      },
      "source": [
        "# shape\n",
        "print('Shape of X:', X.shape)\n",
        "print('Shape of y:', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxDEOqKBiGHr"
      },
      "source": [
        "# TO DO: split training and test data with train_test_split - 1 line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5mtk5IOUQMt"
      },
      "source": [
        "# $$\\textbf{Brief Review of Logistic Regression}$$\n",
        "#### N: number of samples\n",
        "#### f: number of features\n",
        "#### X: data; shape = (N, f)\n",
        "#### y: labels/targets; shape = (N, )\n",
        "#### W: weights; shape = (f, )\n",
        "#### b: bias; scalar\n",
        "#### a: learning rate; scalar\n",
        "\n",
        "## $$\\textbf{Our hypothesis}$$\n",
        "### $$\\hat{y}(X)=\\sigma{(XW+b)}$$\n",
        "\n",
        "## $$\\textbf{Loss function - Binary Cross Entropy}$$\n",
        "### $$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N}\\big(y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})\\big) $$\n",
        "\n",
        "## $$\\textbf{Gradients of Loss with respect to W, b}$$\n",
        "### $$ \\frac{\\partial L}{\\partial W} = \\frac{2}{N}X^T\\big(\\hat{y}-y\\big)$$\n",
        "### $$ \\frac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum_{i=1}^{N}\\big(\\hat{y_i}-y_i\\big)$$\n",
        "\n",
        "## $$\\textbf{Gradient Descent - update W, b accordingly}$$\n",
        "### $$W = W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
        "### $$b = b - \\alpha \\frac{\\partial L}{\\partial b}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_afPZS7KiQ7G"
      },
      "source": [
        "# Logistic Regression Class\n",
        "class Logistic_Regression():\n",
        "  def __init__(self, num_epochs=2000, lr=0.6, verbose=False):\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Logistic_Regression fits a linear model with coefficients w = (w1, ..., wf)\n",
        "    to minimize the binary cross entropy loss between the observed targets in\n",
        "    the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "    Parameters:\n",
        "      num_epochs - number of epochs\n",
        "      lr - learning rate\n",
        "      verbose - creates a plot if set to True\n",
        "\n",
        "    Attributes:\n",
        "      epochs - number of epochs\n",
        "      alpha - learning rate\n",
        "      verbose - creates a plot if set to True\n",
        "      weights - numpy.ndarray with shape (f, )\n",
        "      bias - float\n",
        "      losses - list containing the loss of each epoch\n",
        "    \"\"\"\n",
        "    self.epochs = num_epochs\n",
        "    self.alpha = lr\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def init_params(self, X):\n",
        "    \"\"\"\n",
        "    Initialize the parameters of the model based on input data.\n",
        "\n",
        "    Parameters:\n",
        "      X - data: numpy.ndarray with shape (N, f)\n",
        "\n",
        "    Returns:\n",
        "      nothing\n",
        "    \"\"\"\n",
        "    self.weights = np.random.rand(X.shape[1])\n",
        "    self.bias = np.random.rand()\n",
        "    self.losses = []\n",
        "  \n",
        "  def activate(self, z):\n",
        "    \"\"\"\n",
        "    Activate the outputs of your model using the sigmoid function\n",
        "\n",
        "    Parameters:\n",
        "      z - outputs: numpy.ndarray with shape (N, )\n",
        "\n",
        "    Returns:\n",
        "      activations - numpy.ndarray with shape (N, )\n",
        "    \"\"\"\n",
        "    # TO DO: implement sigmoid activation function - 1 line\n",
        "    activations = None\n",
        "    return activations\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Makes a prediction using the hypothesis.\n",
        "\n",
        "    Parameters:\n",
        "      X - data: numpy.ndarray with shape (N, f)\n",
        "\n",
        "    Returns:\n",
        "      predictions - numpy.ndarray with shape (N, )\n",
        "    \"\"\"\n",
        "    # TO DO: implement predict (using our hypothesis) - 1 line\n",
        "    predictions = None\n",
        "    return self.activate(predictions)\n",
        "\n",
        "  def loss(self, y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes the Binary Cross Entropy Loss of the current predictions.\n",
        "\n",
        "    Parameters:\n",
        "      y - targets: numpy.ndarray with shape (N, )\n",
        "      y_hat - predictions: numpy.ndarray with shape (N, )\n",
        "\n",
        "    Returns:\n",
        "      loss - float\n",
        "    \"\"\"\n",
        "    # TO DO: compute loss (Binary Cross Entropy) - 1 line\n",
        "    log_loss = None\n",
        "    return log_loss\n",
        "\n",
        "  def grads(self, X, y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes the gradients of the loss with respect the weights and bias.\n",
        "\n",
        "    Parameters:\n",
        "      X - data: numpy.ndarray with shape (N, f)\n",
        "      y - targets: numpy.ndarray with shape (N, )\n",
        "      y_hat - predictions: numpy.ndarray with shape (N, )\n",
        "\n",
        "    Returns:\n",
        "      grads - tuple containing the gradients dw, db\n",
        "    \"\"\"\n",
        "    n = y.shape[0]\n",
        "    # TO DO: compute gradients - 2 lines\n",
        "    dw = None\n",
        "    db = None\n",
        "    return dw, db\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fits the model to the training data and targets.\n",
        "\n",
        "    Parameters:\n",
        "      X - training data: numpy.ndarray with shape (N, f)\n",
        "      y - targets: numpy.ndarray with shape (N, )\n",
        "\n",
        "    Returns:\n",
        "      a reference to self\n",
        "    \"\"\"\n",
        "    self.init_params(X)\n",
        "    # main loop for training\n",
        "    for i in range(self.epochs):\n",
        "      # TO DO: implement the training routine - about 5 lines\n",
        "      \n",
        "      \n",
        "      self.losses.append(loss)\n",
        "\n",
        "    if self.verbose:\n",
        "      plt.figure(figsize=[8, 6])\n",
        "      plt.xlabel('epoch')\n",
        "      plt.ylabel('loss')\n",
        "      plt.plot(np.arange(1, len(self.losses) + 1), self.losses)\n",
        "      plt.show()\n",
        "\n",
        "    return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mtVD6bPMlhL"
      },
      "source": [
        "log_reg = Logistic_Regression(verbose=True).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSI-74tOYyU"
      },
      "source": [
        "print('weights:', log_reg.weights)\n",
        "print('bias:', log_reg.bias.round(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RpbkHLBY7pH"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOBLWJPaXxAG"
      },
      "source": [
        "# plot original data\n",
        "y_pred = log_reg.predict(X)\n",
        "plt.scatter(x11, x12, label='Class 0')\n",
        "plt.scatter(x21, x22, label='Class 1')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Ek7AkLYSUW"
      },
      "source": [
        "# plot what our model predicts\n",
        "plt.scatter(X[y_pred < 0.5][:, 0], X[y_pred < 0.5][:, 1], label='pred blue')\n",
        "plt.scatter(X[y_pred > 0.5][:, 0], X[y_pred > 0.5][:, 1], label='pred orange')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE-Rmn8oPC-S"
      },
      "source": [
        "# plot decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x11, x12, label='Class 0')\n",
        "plt.scatter(x21, x22, label='Class 1')\n",
        "\n",
        "slope = - (log_reg.weights[0]/log_reg.weights[1])\n",
        "intercept = - (log_reg.bias/log_reg.weights[1])\n",
        "X_plot = np.arange(2, 7)\n",
        "y_plot = slope * X_plot + intercept\n",
        "plt.plot(X_plot, y_plot, c='black', label='decision boundary')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNQ0HQiBZQbi"
      },
      "source": [
        "# Which accuracy do you expect to be higher?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKrQkSVYXTtB"
      },
      "source": [
        "# TO DO: compute train accuracy - 2 lines\n",
        "y_pred = log_reg.predict(X_train)\n",
        "y_pred = y_pred > 0.5\n",
        "print('Train Accuracy:', accuracy_score(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynQ12FZcP1di"
      },
      "source": [
        "# TO DO: compute test accuracy - 2 lines\n",
        "y_pred = log_reg.predict(X_test)\n",
        "y_pred = y_pred > 0.5\n",
        "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31QlJJL6avkz"
      },
      "source": [
        "# Multi Class Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8ItOFIa0Zq"
      },
      "source": [
        "# additional imports\n",
        "from sklearn.datasets import load_wine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im5nY57uGG4r"
      },
      "source": [
        "# TO DO: Load Data - 1 line\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRXj9xCIGjf4"
      },
      "source": [
        "# examine Data\n",
        "X = X[:, :12]\n",
        "print(\"X: {} samples, {} attributes \".format(X.shape[0], X.shape[1]))\n",
        "print(\"y: {} samples, {} classes\".format(y.shape[0], np.unique(y)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "418E8nJFedne"
      },
      "source": [
        "# Reformat y\n",
        "num_samples = X.shape[0]\n",
        "num_classes = np.unique(y).size\n",
        "y_new = np.zeros((num_samples, num_classes), dtype=np.float64)\n",
        "for idx, e in np.ndenumerate(y):\n",
        "  y_new[idx][e] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnyj6d0QGkPH"
      },
      "source": [
        "# TO DO: split data - 1 line\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC7oxfyfNd8H"
      },
      "source": [
        "# $$\\textbf{Brief Review of Multiclass Classification}$$\n",
        "## Most of this is the same\n",
        "#### N: number of samples\n",
        "#### f: number of features\n",
        "#### c: number of classes\n",
        "#### X: data; shape = (N, f)\n",
        "#### y: labels/targets; shape = (N, c)\n",
        "#### W: weights; shape = (f, c)\n",
        "#### b: bias; shape = (c, )\n",
        "#### a: learning rate; scalar\n",
        "\n",
        "## $$\\textbf{Our hypothesis}$$\n",
        "### $$\\hat{y}(X)= softmax{(XW+b)}$$\n",
        "\n",
        "## $$\\textbf{Loss function - Cross Entropy}$$\n",
        "### $$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{j=1}^{N}\\sum_{i=1}^{c} y_{ji} \\log{\\hat{y_{ji}}}$$\n",
        "\n",
        "## $$\\textbf{Gradients of Loss with respect to W, b (SAME)}$$\n",
        "### $$ \\frac{\\partial L}{\\partial W} = \\frac{2}{N}X^T\\big(\\hat{y}-y\\big)$$\n",
        "### $$ \\frac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum_{i=1}^{N}\\big(\\hat{y_i}-y_i\\big)$$\n",
        "\n",
        "## $$\\textbf{Gradient Descent - update W, b accordingly (SAME)}$$\n",
        "### $$W = W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
        "### $$b = b - \\alpha \\frac{\\partial L}{\\partial b}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVioPgwbNc8y"
      },
      "source": [
        "# Logistic Regression Class\n",
        "\n",
        "class Classifer(Logistic_Regression):\n",
        "  \"\"\"\n",
        "    Multiclass classifier using Softmax and Cross Entropy Loss.\n",
        "\n",
        "    This classifier fits a linear model with coefficients w = (w1, ..., wf)\n",
        "    to minimize the binary cross entropy loss between the observed targets in\n",
        "    the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "    Parameters:\n",
        "      num_classes - number of classes\n",
        "      num_epochs - number of epochs\n",
        "      lr - learning rate\n",
        "      verbose - creates a plot if set to True\n",
        "\n",
        "    Attributes:\n",
        "      epochs - number of epochs\n",
        "      alpha - learning rate\n",
        "      verbose - creates a plot if set to True\n",
        "      weights - numpy.ndarray with shape (f, c)\n",
        "      bias - (c)\n",
        "      losses - list containing the loss of each epoch\n",
        "    \"\"\"\n",
        "  def __init__(self, num_classes, num_epochs=2000, lr=0.6, verbose=False):\n",
        "    self.num_classes = num_classes\n",
        "    self.epochs = num_epochs\n",
        "    self.alpha = lr\n",
        "    self.verbose = verbose\n",
        "    \n",
        "  def init_params(self, X):\n",
        "    \"\"\"\n",
        "    Initialize the parameters of the model based on input data.\n",
        "\n",
        "    Parameters:\n",
        "      X - data: numpy.ndarray with shape (N, f)\n",
        "\n",
        "    Returns:\n",
        "      nothing\n",
        "    \"\"\"\n",
        "    self.weights = np.random.rand(X.shape[1], self.num_classes)\n",
        "    self.bias = np.random.rand(self.num_classes)\n",
        "    self.losses = []\n",
        "  \n",
        "  def activate(self, z):\n",
        "    \"\"\"\n",
        "    Activate the outputs of your model using the sigmoid function\n",
        "\n",
        "    Parameters:\n",
        "      z - outputs: numpy.ndarray with shape (N, c)\n",
        "\n",
        "    Returns:\n",
        "      activations - numpy.ndarray with shape (N, c)\n",
        "    \"\"\"\n",
        "    # TO DO: implement softmax activation function - 1-2 line\n",
        "    \n",
        "\n",
        "    return activations\n",
        "\n",
        "  def loss(self, y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes the Binary Cross Entropy Loss of the current predictions.\n",
        "\n",
        "    Parameters:\n",
        "      y - targets: numpy.ndarray with shape (N, c)\n",
        "      y_hat - predictions: numpy.ndarray with shape (N, c)\n",
        "\n",
        "    Returns:\n",
        "      loss - float\n",
        "    \"\"\"\n",
        "    # TO DO: compute loss (Cross Entropy) - 1 line\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def test(self, X, y): \n",
        "    \"\"\"\n",
        "    Test accuracy on a set of data and targets\n",
        "\n",
        "    Parameters:\n",
        "      X - data: np.ndarray with shape(N, f)\n",
        "      y - targets: np.ndarray with same (N, c)\n",
        "    \"\"\"\n",
        "    # TO DO - test model on test data - 3-4 lines\n",
        "    # get predictions, go from 1 hot encodings -> class values (vector to numbers), test accuracy with accuracy_score\n",
        "    # recommended function: np.argmax(..., axis=1)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    return accuracy_score(preds, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqyzL5dPL9fd"
      },
      "source": [
        "# TO DO: Fit on training data with 3 classes, 7000 epochs, lr ~= 0.001 verbose=True - 1 line\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnhu-gMhp2SS"
      },
      "source": [
        "# TO DO: Check accuracy on training and test data - 2 lines\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}